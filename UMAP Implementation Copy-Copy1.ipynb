{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mnist in /home/adalmia1/anaconda3/lib/python3.7/site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in /home/adalmia1/anaconda3/lib/python3.7/site-packages (from mnist) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mnist\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numba\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, KernelPCA\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "\n",
    "import mnist\n",
    "from sklearn import datasets,metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pynndescent import NNDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(dens_frac=0.0, dens_lambda=0.0, min_dist=1, n_components=100,\n",
      "     n_neighbors=25, verbose=True)\n",
      "Construct fuzzy simplicial set\n",
      "Sun Feb  7 08:27:19 2021 Finding Nearest Neighbors\n",
      "Sun Feb  7 08:27:20 2021 Building RP forest with 12 trees\n",
      "Sun Feb  7 08:27:34 2021 NN descent for 14 iterations\n",
      "\t 1  /  14\n",
      "\t 2  /  14\n",
      "\t 3  /  14\n",
      "\t 4  /  14\n",
      "\t 5  /  14\n",
      "\t 6  /  14\n",
      "\t 7  /  14\n",
      "\tStopping threshold met -- exiting after 7 iterations\n",
      "Sun Feb  7 08:30:11 2021 Finished Nearest Neighbor Search\n",
      "Sun Feb  7 08:30:12 2021 Construct embedding\n",
      "\tcompleted  0  /  200 epochs\n",
      "\tcompleted  20  /  200 epochs\n",
      "\tcompleted  40  /  200 epochs\n",
      "\tcompleted  60  /  200 epochs\n",
      "\tcompleted  80  /  200 epochs\n",
      "\tcompleted  100  /  200 epochs\n",
      "\tcompleted  120  /  200 epochs\n",
      "\tcompleted  140  /  200 epochs\n",
      "\tcompleted  160  /  200 epochs\n",
      "\tcompleted  180  /  200 epochs\n",
      "Sun Feb  7 08:30:49 2021 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "# Load full MNIST dataset\n",
    "MNIST_X_train = mnist.train_images()\n",
    "MNIST_n_samples = len(MNIST_X_train)\n",
    "MNIST_X_train = MNIST_X_train.reshape((len(MNIST_X_train), -1)) \n",
    "MNIST_y_train = mnist.train_labels()\n",
    "\n",
    "\n",
    "# Load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "digits_n_samples = len(digits.images)\n",
    "digits_X_train = digits.images.reshape((digits_n_samples, -1))\n",
    "digits_y_train = digits.target\n",
    "\n",
    "\n",
    "#Load 20 NewsGroup\n",
    "vectorizer = TfidfVectorizer(min_df=5, stop_words='english') \n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='all')\n",
    "newsgroups_X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "newsgroups_n_train = len(newsgroups_X_train)\n",
    "newsgroups_y_train = newsgroups_train.target\n",
    "\n",
    "\n",
    "from umap import UMAP\n",
    "model = UMAP(n_neighbors = 25, min_dist = 1, n_components = 100, verbose = True)\n",
    "newsgroups_X_train = model.fit_transform(newsgroups_X_train)\n",
    "\n",
    "\n",
    "\n",
    "n = newsgroups_n_train\n",
    "X_train = newsgroups_X_train\n",
    "y_train = newsgroups_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate a,b hyperparams given the MIN_DIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters a = 1.9325904510795369 and b = 0.7903232192799697\n"
     ]
    }
   ],
   "source": [
    "def find_ab(MIN_DIST=0):\n",
    "    x = np.linspace(0, 3, 1000)\n",
    "  \n",
    "    def f(x, min_dist):\n",
    "        y = []\n",
    "        for i in range(len(x)):\n",
    "            if(x[i] <= min_dist):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(np.exp(- x[i] + min_dist))\n",
    "        return y\n",
    "\n",
    "    dist_low_dim = lambda x, a, b: 1 / (1 + a*x**(2*b))\n",
    "\n",
    "    p , _ = optimize.curve_fit(dist_low_dim, x, f(x, MIN_DIST))\n",
    "\n",
    "    a = p[0]\n",
    "    b = p[1] \n",
    "    print(\"Hyperparameters a = \" + str(a) + \" and b = \" + str(b))\n",
    "    return a, b\n",
    "a, b = find_ab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the approximate nearest neighbor and new distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Nearest Neighbors\n",
      "Sun Feb  7 08:30:54 2021 Building RP forest with 12 trees\n",
      "Sun Feb  7 08:30:54 2021 NN descent for 14 iterations\n",
      "\t 1  /  14\n",
      "\t 2  /  14\n",
      "\t 3  /  14\n",
      "\t 4  /  14\n",
      "\tStopping threshold met -- exiting after 4 iterations\n",
      "Finished Nearest Neighbor Search\n"
     ]
    }
   ],
   "source": [
    "def nearest_neighbors(X, n_neighbors=10):\n",
    "    \"\"\"Compute the ``n_neighbors`` nearest points for each data point in ``X``\n",
    "    under ``metric``. This may be exact, but more likely is approximated via\n",
    "    nearest neighbor descent. \"\"\"\n",
    "    print(\"Finding Nearest Neighbors\")\n",
    "    n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0)))\n",
    "    n_iters = max(5, int(round(np.log2(X.shape[0]))))\n",
    "\n",
    "    knn_search_index = NNDescent(X, n_neighbors=n_neighbors, metric='euclidean', metric_kwds={}, random_state=None, n_trees=n_trees, n_iters=n_iters, max_candidates=60,low_memory=True, n_jobs= 4,verbose=True)\n",
    "\n",
    "    knn_indices = knn_search_index._neighbor_graph[0].copy()\n",
    "    knn_dists = knn_search_index._neighbor_graph[1].copy()\n",
    "    \n",
    "    print(\"Finished Nearest Neighbor Search\")\n",
    "    return knn_indices, knn_dists, knn_search_index\n",
    "\n",
    "  \n",
    "knn_indices, knn_dists, knn_search_index = nearest_neighbors(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Fuzzy Simplicial Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating rho and sigma\n",
      "\tcompleted  0  /  18846 epochs\n",
      "\tcompleted  1884  /  18846 epochs\n",
      "\tcompleted  3768  /  18846 epochs\n",
      "\tcompleted  5652  /  18846 epochs\n",
      "\tcompleted  7536  /  18846 epochs\n",
      "\tcompleted  9420  /  18846 epochs\n",
      "\tcompleted  11304  /  18846 epochs\n",
      "\tcompleted  13188  /  18846 epochs\n",
      "\tcompleted  15072  /  18846 epochs\n",
      "\tcompleted  16956  /  18846 epochs\n",
      "\tcompleted  18840  /  18846 epochs\n"
     ]
    }
   ],
   "source": [
    "def smooth_knn_dist(distances, n_neighbors=10, n_iter=64):\n",
    "    \"\"\"Compute a continuous version of the distance to the kth nearest\n",
    "    neighbor. That is, this is similar to knn-distance but allows continuous\n",
    "    k values rather than requiring an integral k. In essence we are simply\n",
    "    computing the distance such that the cardinality of fuzzy set we generate\n",
    "    is k.\n",
    "    \"\"\"\n",
    "    \n",
    "    target = np.log2(n_neighbors)\n",
    "    rho = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "    sigma = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "    \n",
    "    print(\"Calculating rho and sigma\")\n",
    "    for i in range(distances.shape[0]):\n",
    "        rho[i] = distances[i][1]\n",
    "        \n",
    "        lo = 0.0\n",
    "        hi = np.inf\n",
    "        mid = 1.0\n",
    "        \n",
    "        for n in range(n_iter):\n",
    "            \n",
    "            #Calculate probability\n",
    "            psum = 0.0\n",
    "            for j in range(1, distances.shape[1]):\n",
    "                d = distances[i, j] - rho[i]\n",
    "                psum += np.exp(-(d / mid)) if d > 0 else 1.0 \n",
    "                \n",
    "            #binary search to find sigma\n",
    "            if np.fabs(psum - target) < 1e-5: #SMOOTH_K_TOLERANCE\n",
    "                break\n",
    "            if psum > target:\n",
    "                hi = mid\n",
    "                mid = (lo + hi) / 2.0\n",
    "            else:\n",
    "                lo = mid\n",
    "                if hi == np.inf:\n",
    "                    mid *= 2\n",
    "                else:\n",
    "                    mid = (lo + hi) / 2.0\n",
    "\n",
    "        sigma[i] = mid\n",
    "        \n",
    "        if i % int(distances.shape[0] / 10) == 0:\n",
    "            print(\"\\tcompleted \", i, \" / \", distances.shape[0], \"epochs\")\n",
    "            \n",
    "\n",
    "    return sigma, rho\n",
    "\n",
    "\n",
    "def compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos, bipartite=False):\n",
    "    \"\"\"Construct the membership strength data for the 1-skeleton of each local\n",
    "    fuzzy simplicial set -- this is formed as a sparse matrix where each row is\n",
    "    a local fuzzy simplicial set, with a membership strength for the\n",
    "    1-simplex to each other data point.\n",
    "    \"\"\"\n",
    "    n_samples = knn_indices.shape[0]\n",
    "    n_neighbors = knn_indices.shape[1]\n",
    "\n",
    "    rows = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    cols = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    vals = np.zeros(knn_indices.size, dtype=np.float32)\n",
    "    dists = np.zeros(knn_indices.size, dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_neighbors):\n",
    "            if (bipartite == False) & (knn_indices[i, j] == i):\n",
    "                val = 0.0\n",
    "            elif knn_dists[i, j] - rhos[i] <= 0.0 or sigmas[i] == 0.0:\n",
    "                val = 1.0\n",
    "            else:\n",
    "                val = np.exp(-((knn_dists[i, j] - rhos[i]) / (sigmas[i])))\n",
    "            rows[i * n_neighbors + j] = i\n",
    "            cols[i * n_neighbors + j] = knn_indices[i, j]\n",
    "            vals[i * n_neighbors + j] = val\n",
    "            dists[i * n_neighbors + j] = knn_dists[i, j]\n",
    "\n",
    "    return rows, cols, vals, dists\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def fuzzy_simplicial_set(X, knn_indices, knn_dists, n_neighbors=25):\n",
    "    \"\"\"Given a set of data X, a neighborhood size, and a measure of distance\n",
    "    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n",
    "    the form of a sparse matrix) associated to the data. This is done by\n",
    "    locally approximating geodesic distance at each point, creating a fuzzy\n",
    "    simplicial set for each such point, and then combining all the local\n",
    "    fuzzy simplicial sets into a global one via a fuzzy union.\n",
    "    \"\"\"\n",
    "    \n",
    "    knn_dists = knn_dists.astype(np.float32)\n",
    "    sigmas, rhos = smooth_knn_dist(knn_dists)\n",
    "    \n",
    "    \n",
    "    rows, cols, vals, dists = compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos)\n",
    "\n",
    "    #Calculate the fuzzy_simplicial_set\n",
    "    Pij = scipy.sparse.coo_matrix((vals, (rows, cols)), shape=(X.shape[0], X.shape[0]))\n",
    "    Pij.eliminate_zeros()\n",
    "    \n",
    "    Pji = Pij.transpose()\n",
    "    prod_matrix = Pij.multiply(Pji) \n",
    "    P = (Pij + Pji - prod_matrix)\n",
    "    P.eliminate_zeros()\n",
    "    \n",
    "    #Calculate the fuzzy simplex distance matrix\n",
    "    dmat = scipy.sparse.coo_matrix((dists, (rows, cols)), shape=(X.shape[0], X.shape[0]))\n",
    "    dists = dmat.maximum(dmat.transpose()).todok()\n",
    "\n",
    "    return P, sigmas, rhos, dists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P, sigmas, rhos, dists = fuzzy_simplicial_set(X_train, knn_indices, knn_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_tfidf_layout(data, dim):\n",
    "  #vectorizer = TfidfVectorizer(min_df=5, stop_words='english') \n",
    "  #data = fetch_20newsgroups(subset='train')\n",
    "  #newsgroups_X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "  \n",
    "  svd_model = TruncatedSVD(n_components=dim, algorithm='randomized', n_iter=100, random_state=122)\n",
    "  svd_data = svd_model.fit_transform(data)\n",
    "  \n",
    "  return svd_data\n",
    "  \n",
    "def spectral_layout(data, graph, dim):\n",
    "    \"\"\"Given a graph compute the spectral embedding of the graph. This is\n",
    "    simply the eigenvectors of the laplacian of the graph. Here we use the\n",
    "    normalized laplacian.\n",
    "    \"\"\"\n",
    "    diag_data = np.asarray(graph.sum(axis=0))\n",
    "    # Normalized Laplacian\n",
    "    I = scipy.sparse.identity(graph.shape[0], dtype=np.float64)\n",
    "    D = scipy.sparse.spdiags(1.0 / np.sqrt(diag_data), 0, graph.shape[0], graph.shape[0])\n",
    "    L = I - D * graph * D\n",
    "    k = dim + 1\n",
    "    eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(L, k, which=\"SM\")\n",
    "    order = np.argsort(eigenvalues)[1:k]\n",
    "    return eigenvectors[:, order]\n",
    "\n",
    "def random_layout(data, graph, dim):\n",
    "    random_state = np.random.RandomState(1234)\n",
    "    return random_state.uniform(low=-10.0, high=10.0, size=(graph.shape[0], dim))\n",
    "  \n",
    "def init_embeddings(X, P, n_components):\n",
    "  \n",
    "  # init\n",
    "  initialisation = spectral_layout(X, P, n_components)\n",
    "  #Add noise\n",
    "  expansion = 10.0 / np.abs(initialisation).max()\n",
    "  random_state = check_random_state(0)\n",
    "  embedding = (initialisation * expansion).astype(np.float32) + random_state.normal(scale=0.0001, size=[P.shape[0], n_components]).astype(np.float32)\n",
    "  \n",
    "  embedding = (10.0 * (embedding - np.min(embedding, 0)) / (np.max(embedding, 0) - np.min(embedding, 0))).astype(np.float32, order=\"C\")\n",
    "  return embedding\n",
    "\n",
    "\n",
    "#tfidf_layout(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArpackNoConvergence",
     "evalue": "ARPACK error -1: No convergence (188461 iterations, 0/3 eigenvectors converged)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArpackNoConvergence\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3456cb2eec93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimplicial_set_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-3456cb2eec93>\u001b[0m in \u001b[0;36msimplicial_set_embedding\u001b[0;34m(X, P, n_components, initial_alpha, a, b, gamma, negative_sample_rate)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m#Make epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-a1e35dc6eea2>\u001b[0m in \u001b[0;36minit_embeddings\u001b[0;34m(X, P, n_components)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0minitialisation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectral_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0;31m#Add noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mexpansion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitialisation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-a1e35dc6eea2>\u001b[0m in \u001b[0;36mspectral_layout\u001b[0;34m(data, graph, dim)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mI\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0meigenvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigenvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigsh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigenvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meigenvectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36meigsh\u001b[0;34m(A, k, M, sigma, which, v0, ncv, maxiter, tol, return_eigenvectors, Minv, OPinv, mode)\u001b[0m\n\u001b[1;32m   1685\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_ARPACK_LOCK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_eigenvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36miterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_no_convergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mArpackError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfodict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate_infodict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/eigen/arpack/arpack.py\u001b[0m in \u001b[0;36m_raise_no_convergence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mk_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mArpackNoConvergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_ok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArpackNoConvergence\u001b[0m: ARPACK error -1: No convergence (188461 iterations, 0/3 eigenvectors converged)"
     ]
    }
   ],
   "source": [
    "INT32_MIN = np.iinfo(np.int32).min + 1\n",
    "INT32_MAX = np.iinfo(np.int32).max - 1\n",
    "\n",
    "\n",
    "def make_epochs_per_sample(weights, n_epochs):\n",
    "    \"\"\"Given a set of weights and number of epochs generate the number of\n",
    "    epochs per sample for each weight.\n",
    "    Returns\n",
    "    -------\n",
    "    An array of number of epochs per sample, one for each 1-simplex.\n",
    "    \"\"\"\n",
    "    result = -1.0 * np.ones(weights.shape[0], dtype=np.float64)\n",
    "    n_samples = n_epochs * (weights / weights.max())\n",
    "    \n",
    "    result[n_samples > 0] = float(n_epochs) / n_samples[n_samples > 0]\n",
    "    print(result.shape)\n",
    "    return result\n",
    "\n",
    "  \n",
    "@numba.njit()\n",
    "def clip(val):\n",
    "    \"\"\"Standard clamping of a value into a fixed range (in this case -4.0 to\n",
    "    4.0)\n",
    "    \"\"\"\n",
    "    if val > 4.0:\n",
    "        return 4.0\n",
    "    elif val < -4.0:\n",
    "        return -4.0\n",
    "    else:\n",
    "      return val  \n",
    "  \n",
    "\n",
    "  \n",
    "@numba.njit(\n",
    "    \"f4(f4[::1],f4[::1])\",\n",
    "    fastmath=True,\n",
    "    cache=True,\n",
    "    locals={\n",
    "        \"result\": numba.types.float32,\n",
    "        \"diff\": numba.types.float32,\n",
    "        \"dim\": numba.types.int32,\n",
    "    },\n",
    ")\n",
    "def rdist(x, y):\n",
    "    result = 0.0\n",
    "    dim = x.shape[0]\n",
    "    for i in range(dim):\n",
    "        diff = x[i] - y[i]\n",
    "        result += diff * diff\n",
    "\n",
    "    return result\n",
    "  \n",
    "  \n",
    "  \n",
    "@numba.njit(\"i4(i8[:])\")\n",
    "def tau_rand_int(state):\n",
    "    \"\"\"A fast (pseudo)-random number generator.\n",
    "    A (pseudo)-random int32 value\n",
    "    \"\"\"\n",
    "    state[0] = (((state[0] & 4294967294) << 12) & 0xFFFFFFFF) ^ (\n",
    "        (((state[0] << 13) & 0xFFFFFFFF) ^ state[0]) >> 19\n",
    "    )\n",
    "    state[1] = (((state[1] & 4294967288) << 4) & 0xFFFFFFFF) ^ (\n",
    "        (((state[1] << 2) & 0xFFFFFFFF) ^ state[1]) >> 25\n",
    "    )\n",
    "    state[2] = (((state[2] & 4294967280) << 17) & 0xFFFFFFFF) ^ (\n",
    "        (((state[2] << 3) & 0xFFFFFFFF) ^ state[2]) >> 11\n",
    "    )\n",
    "\n",
    "    return state[0] ^ state[1] ^ state[2]\n",
    "\n",
    "  \n",
    "def optimize_layout_euclidean_single_epoch(head_embedding, tail_embedding, head,tail,n_vertices,epochs_per_sample,a,b,rng_state,dim,move_other,alpha,epochs_per_negative_sample,epoch_of_next_negative_sample,epoch_of_next_sample,n):\n",
    "    for i in numba.prange(epochs_per_sample.shape[0]):\n",
    "        if epoch_of_next_sample[i] <= n:\n",
    "            j = head[i]\n",
    "            k = tail[i]\n",
    "\n",
    "            current = head_embedding[j]\n",
    "            other = tail_embedding[k]\n",
    "\n",
    "            \n",
    "            dist_squared = rdist(current, other)\n",
    "            \n",
    "            grad_coeff = -2.0 * a * b * pow(dist_squared, b - 1.0) \n",
    "            grad_coeff /= (a*pow(dist_squared, b) + 1.0)\n",
    "\n",
    "            for d in range(dim): \n",
    "                grad_d = clip(grad_coeff * (current[d] - other[d]))\n",
    "                current[d] += grad_d * alpha\n",
    "                if move_other:\n",
    "                    other[d] += -grad_d * alpha\n",
    "\n",
    "            epoch_of_next_sample[i] += epochs_per_sample[i]\n",
    "\n",
    "            n_neg_samples = int((n - epoch_of_next_negative_sample[i]) / epochs_per_negative_sample[i])\n",
    "\n",
    "            for p in range(n_neg_samples):\n",
    "              \n",
    "                k = tau_rand_int(rng_state) % n_vertices\n",
    "                other = tail_embedding[k]\n",
    "                dist_squared = rdist(current, other)\n",
    "\n",
    "                if dist_squared > 0.0:\n",
    "                    grad_coeff = 2.0  * b \n",
    "                    grad_coeff /= (0.001 + dist_squared) * (a * pow(dist_squared, b) + 1)\n",
    "                elif j == k:\n",
    "                    continue\n",
    "                else:\n",
    "                    grad_coeff = 0.0\n",
    "\n",
    "                for d in range(dim):\n",
    "                    if grad_coeff > 0.0:\n",
    "                        grad_d = clip(grad_coeff * (current[d] - other[d]))\n",
    "                    else:\n",
    "                        grad_d = 4.0\n",
    "                    current[d] += grad_d * alpha\n",
    "\n",
    "            epoch_of_next_negative_sample[i] += (n_neg_samples * epochs_per_negative_sample[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def simplicial_set_embedding(X, P, n_components, initial_alpha, a, b, gamma, negative_sample_rate):\n",
    "    \"\"\"Perform a fuzzy simplicial set embedding, using a specified\n",
    "    initialisation method and then minimizing the fuzzy set cross entropy\n",
    "    between the 1-skeletons of the high and low dimensional fuzzy simplicial\n",
    "    sets.\n",
    "    \"\"\"\n",
    "    graph = P.tocoo()\n",
    "    graph.sum_duplicates()\n",
    "    n_vertices = graph.shape[1]\n",
    "\n",
    "    n_epochs = 200\n",
    "    \n",
    "    embedding = init_embeddings(X_train, P, n_components)\n",
    "    \n",
    "    #Make epochs\n",
    "    epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "   \n",
    "    \n",
    "    head = graph.row\n",
    "    tail = graph.col\n",
    "    weight = graph.data\n",
    "    \n",
    "    random_state = check_random_state(0)\n",
    "    rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "    \n",
    "    #Optimize Step\n",
    "    head_embedding = embedding\n",
    "    tail_embedding = embedding\n",
    "    \n",
    "    \n",
    "    move_other = head_embedding.shape[0] == tail_embedding.shape[0]\n",
    "    \n",
    "    \n",
    "    epochs_per_negative_sample = epochs_per_sample / negative_sample_rate\n",
    "    epoch_of_next_negative_sample = epochs_per_negative_sample.copy()\n",
    "    \n",
    "    #print(epoch_of_next_negative_sample)\n",
    "    \n",
    "    epoch_of_next_sample = epochs_per_sample.copy()\n",
    "    #print(epoch_of_next_sample)\n",
    "\n",
    "    \n",
    "    optimize_fn = numba.njit(optimize_layout_euclidean_single_epoch, fastmath=True, parallel=False)\n",
    "    \n",
    "    alpha = initial_alpha\n",
    "    \n",
    "\n",
    "    for n in range(n_epochs):\n",
    "\n",
    "\n",
    "        optimize_fn(\n",
    "            head_embedding,\n",
    "            tail_embedding,\n",
    "            head,\n",
    "            tail,\n",
    "            n_vertices,\n",
    "            epochs_per_sample,\n",
    "            a,\n",
    "            b,\n",
    "            rng_state,\n",
    "            n_components,\n",
    "            move_other,\n",
    "            alpha,\n",
    "            epochs_per_negative_sample,\n",
    "            epoch_of_next_negative_sample,\n",
    "            epoch_of_next_sample,\n",
    "            n\n",
    "        )\n",
    "\n",
    "        alpha = initial_alpha * (1.0 - (float(n) / float(n_epochs)))\n",
    "\n",
    "        if n % int(n_epochs / 10) == 0:\n",
    "            print(\"\\tcompleted \", n, \" / \", n_epochs, \"epochs\")\n",
    "\n",
    "    return head_embedding\n",
    "\n",
    "\n",
    "  \n",
    "embeds = simplicial_set_embedding(X_train, P, 2, 1, a, b, 1, 20)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "colors += cm.get_cmap(\"Set3\").colors\n",
    "colors += cm.get_cmap(\"Set2\").colors\n",
    "my_cmap = ListedColormap(colors)\n",
    "\n",
    "plt.scatter(embeds[:, 0], embeds[:, 1], c = y_train.astype(int), cmap = my_cmap, s = 10)\n",
    "plt.title('UMAP', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=2)\n",
    "y_pred_train = kmeans.fit_predict(embeds)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "def evaluate(y, y_pred): \n",
    "  print(\"Accurary Score: %0.3f\" % purity_score(y, y_pred))\n",
    "  print(\"Adjusted Mutual Information Score: %0.3f\" % metrics.adjusted_mutual_info_score(y, y_pred))\n",
    "  print(\"Adjusted Rand Index Score: %0.3f\" % metrics.adjusted_rand_score(y, y_pred))\n",
    "  print(\"Normalized Mutual Information Score: %0.3f\" % metrics.normalized_mutual_info_score(y, y_pred))\n",
    "\n",
    "  print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, y_pred))\n",
    "  print(\"Completeness: %0.3f\" % metrics.completeness_score(y, y_pred))\n",
    "  print(\"V-measure: %0.3f\" % metrics.v_measure_score(y, y_pred))\n",
    "\n",
    "\n",
    "evaluate(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from umap import UMAP\n",
    "#model = UMAP(n_neighbors = 15, min_dist = 0.1, n_components = 2, verbose = True)\n",
    "#umap = model.fit_transform(X_train)\n",
    "#plt.scatter(umap[:, 0], umap[:, 1], c = y_train.astype(int), cmap = my_cmap, s = 50)\n",
    "#plt.title('UMAP', fontsize = 20)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
