{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mnist in /home/adalmia1/anaconda3/lib/python3.7/site-packages (0.2.2)\r\n",
      "Requirement already satisfied: numpy in /home/adalmia1/anaconda3/lib/python3.7/site-packages (from mnist) (1.19.5)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalmia1/anaconda3/lib/python3.7/site-packages/umap/__init__.py:9: UserWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "  warn(\"Tensorflow not installed; ParametricUMAP will be unavailable\")\n"
     ]
    }
   ],
   "source": [
    "!pip3 install mnist\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numba\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, KernelPCA\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import mnist\n",
    "from sklearn import datasets,metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pynndescent import NNDescent\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full MNIST dataset\n",
    "MNIST_X_train = mnist.train_images()\n",
    "MNIST_n_samples = len(MNIST_X_train)\n",
    "MNIST_X_train = MNIST_X_train.reshape((len(MNIST_X_train), -1)) \n",
    "MNIST_y_train = mnist.train_labels()\n",
    "\n",
    "\n",
    "# Load digits dataset\n",
    "digits = datasets.load_digits()\n",
    "digits_n_samples = len(digits.images)\n",
    "digits_X_train = digits.images.reshape((digits_n_samples, -1))\n",
    "digits_y_train = digits.target\n",
    "\n",
    "\n",
    "#Load 20 NewsGroup\n",
    "vectorizer = TfidfVectorizer(min_df=5, stop_words='english') \n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='all')\n",
    "newsgroups_X_train = vectorizer.fit_transform(newsgroups_train.data).toarray()\n",
    "newsgroups_n_train = len(newsgroups_X_train)\n",
    "newsgroups_y_train = newsgroups_train.target\n",
    "\n",
    "\n",
    "n = newsgroups_n_train\n",
    "X_train = newsgroups_X_train\n",
    "y_train = newsgroups_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate a,b hyperparams given the MIN_DIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters a = 0.114931600873422 and b = 1.9296586344338804\n"
     ]
    }
   ],
   "source": [
    "def find_ab(MIN_DIST=1):\n",
    "    x = np.linspace(0, 3, 1000)\n",
    "  \n",
    "    def f(x, min_dist):\n",
    "        y = []\n",
    "        for i in range(len(x)):\n",
    "            if(x[i] <= min_dist):\n",
    "                y.append(1)\n",
    "            else:\n",
    "                y.append(np.exp(- x[i] + min_dist))\n",
    "        return y\n",
    "\n",
    "    dist_low_dim = lambda x, a, b: 1 / (1 + a*x**(2*b))\n",
    "\n",
    "    p , _ = optimize.curve_fit(dist_low_dim, x, f(x, MIN_DIST))\n",
    "\n",
    "    a = p[0]\n",
    "    b = p[1] \n",
    "    print(\"Hyperparameters a = \" + str(a) + \" and b = \" + str(b))\n",
    "    return a, b\n",
    "a, b = find_ab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the approximate nearest neighbor and new distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Nearest Neighbors\n",
      "UMAP(dens_frac=0.0, dens_lambda=0.0, min_dist=1, n_components=500, n_epochs=800,\n",
      "     n_jobs=1, n_neighbors=25, verbose=True)\n",
      "Construct fuzzy simplicial set\n",
      "Sat Feb  6 12:55:41 2021 Finding Nearest Neighbors\n",
      "Sat Feb  6 12:55:41 2021 Building RP forest with 12 trees\n",
      "Sat Feb  6 12:56:39 2021 NN descent for 14 iterations\n",
      "\t 1  /  14\n",
      "\t 2  /  14\n",
      "\t 3  /  14\n",
      "\t 4  /  14\n",
      "\t 5  /  14\n",
      "\t 6  /  14\n",
      "\t 7  /  14\n",
      "\tStopping threshold met -- exiting after 7 iterations\n",
      "Sat Feb  6 13:04:36 2021 Finished Nearest Neighbor Search\n",
      "Sat Feb  6 13:04:41 2021 Construct embedding\n",
      "\tcompleted  0  /  800 epochs\n",
      "\tcompleted  80  /  800 epochs\n",
      "\tcompleted  160  /  800 epochs\n",
      "\tcompleted  240  /  800 epochs\n",
      "\tcompleted  320  /  800 epochs\n",
      "\tcompleted  400  /  800 epochs\n",
      "\tcompleted  480  /  800 epochs\n",
      "\tcompleted  560  /  800 epochs\n",
      "\tcompleted  640  /  800 epochs\n",
      "\tcompleted  720  /  800 epochs\n",
      "Sat Feb  6 13:23:01 2021 Finished embedding\n",
      "Sat Feb  6 13:23:05 2021 Building RP forest with 12 trees\n",
      "Sat Feb  6 13:23:05 2021 NN descent for 14 iterations\n",
      "\t 1  /  14\n",
      "\t 2  /  14\n",
      "\t 3  /  14\n",
      "\t 4  /  14\n",
      "\tStopping threshold met -- exiting after 4 iterations\n",
      "Finished Nearest Neighbor Search\n"
     ]
    }
   ],
   "source": [
    "def get_dists(indices, X):\n",
    "  ind = []\n",
    "  dist = []\n",
    "  for i, xi in enumerate(X):\n",
    "    group_embeds = X[indices[i]]\n",
    "    dist = np.sum((group_embeds - X[i])**2,axis=1)\n",
    "    dist = np.sqrt(dist)\n",
    "    dist_ind = np.argsort(dist)[:n_neighbors]\n",
    "    dist.append(dist[dist_ind])\n",
    "    ind.append(indices[i][dist_ind])\n",
    "  return ind, dist\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nearest_neighbors(X, n_neighbors=15):\n",
    "    \"\"\"Compute the ``n_neighbors`` nearest points for each data point in ``X``\n",
    "    under ``metric``. This may be exact, but more likely is approximated via\n",
    "    nearest neighbor descent. \"\"\"\n",
    "    print(\"Finding Nearest Neighbors\")\n",
    "    \n",
    "    model = UMAP(n_neighbors = 25, min_dist = 1 , n_jobs =1,  n_components = 500, n_epochs = 800, verbose = True)\n",
    "    Xr = model.fit_transform(X)\n",
    "    \n",
    "    n_trees = min(64, 5 + int(round((X.shape[0]) ** 0.5 / 20.0)))\n",
    "    n_iters = max(5, int(round(np.log2(X.shape[0]))))\n",
    "\n",
    "    knn_search_index = NNDescent(Xr, n_neighbors=n_neighbors, metric='euclidean', metric_kwds={}, random_state=None, n_trees=n_trees, n_iters=n_iters, max_candidates=60,low_memory=True, n_jobs= 4,verbose=True)\n",
    "    \n",
    "    knn_indices = knn_search_index._neighbor_graph[0].copy()\n",
    "    knn_dists = knn_search_index._neighbor_graph[1].copy()\n",
    "    \n",
    "    #knn_dists = np.array(knn_dists)\n",
    "    #knn_indices = np.array(knn_indices)\n",
    "    \n",
    "    \n",
    "    print(\"Finished Nearest Neighbor Search\")\n",
    "    return knn_indices, knn_dists, knn_search_index\n",
    "\n",
    "  \n",
    "knn_indices, knn_dists, knn_search_index = nearest_neighbors(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Fuzzy Simplicial Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating rho and sigma\n",
      "\tcompleted  0  /  18846 epochs\n",
      "\tcompleted  1884  /  18846 epochs\n",
      "\tcompleted  3768  /  18846 epochs\n",
      "\tcompleted  5652  /  18846 epochs\n",
      "\tcompleted  7536  /  18846 epochs\n",
      "\tcompleted  9420  /  18846 epochs\n",
      "\tcompleted  11304  /  18846 epochs\n",
      "\tcompleted  13188  /  18846 epochs\n",
      "\tcompleted  15072  /  18846 epochs\n",
      "\tcompleted  16956  /  18846 epochs\n",
      "\tcompleted  18840  /  18846 epochs\n"
     ]
    }
   ],
   "source": [
    "def smooth_knn_dist(distances, n_neighbors=15, n_iter=64):\n",
    "    \"\"\"Compute a continuous version of the distance to the kth nearest\n",
    "    neighbor. That is, this is similar to knn-distance but allows continuous\n",
    "    k values rather than requiring an integral k. In essence we are simply\n",
    "    computing the distance such that the cardinality of fuzzy set we generate\n",
    "    is k.\n",
    "    \"\"\"\n",
    "    \n",
    "    target = np.log2(n_neighbors)\n",
    "    rho = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "    sigma = np.zeros(distances.shape[0], dtype=np.float32)\n",
    "    \n",
    "    print(\"Calculating rho and sigma\")\n",
    "    for i in range(distances.shape[0]):\n",
    "        rho[i] = distances[i][1]\n",
    "        \n",
    "        lo = 0.0\n",
    "        hi = np.inf\n",
    "        mid = 1.0\n",
    "        \n",
    "        for n in range(n_iter):\n",
    "            \n",
    "            #Calculate probability\n",
    "            psum = 0.0\n",
    "            for j in range(1, distances.shape[1]):\n",
    "                d = distances[i, j] - rho[i]\n",
    "                psum += np.exp(-(d / mid)) if d > 0 else 1.0 \n",
    "                \n",
    "            #binary search to find sigma\n",
    "            if np.fabs(psum - target) < 1e-5: #SMOOTH_K_TOLERANCE\n",
    "                break\n",
    "            if psum > target:\n",
    "                hi = mid\n",
    "                mid = (lo + hi) / 2.0\n",
    "            else:\n",
    "                lo = mid\n",
    "                if hi == np.inf:\n",
    "                    mid *= 2\n",
    "                else:\n",
    "                    mid = (lo + hi) / 2.0\n",
    "        \n",
    "        if i % int(distances.shape[0] / 10) == 0:\n",
    "            print(\"\\tcompleted \", i, \" / \", distances.shape[0], \"epochs\")\n",
    "            \n",
    "        sigma[i] = mid\n",
    "\n",
    "    return sigma, rho\n",
    "\n",
    "\n",
    "def compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos, bipartite=False):\n",
    "    \"\"\"Construct the membership strength data for the 1-skeleton of each local\n",
    "    fuzzy simplicial set -- this is formed as a sparse matrix where each row is\n",
    "    a local fuzzy simplicial set, with a membership strength for the\n",
    "    1-simplex to each other data point.\n",
    "    \"\"\"\n",
    "    n_samples = knn_indices.shape[0]\n",
    "    n_neighbors = knn_indices.shape[1]\n",
    "\n",
    "    rows = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    cols = np.zeros(knn_indices.size, dtype=np.int32)\n",
    "    vals = np.zeros(knn_indices.size, dtype=np.float32)\n",
    "    dists = np.zeros(knn_indices.size, dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_neighbors):\n",
    "            if (bipartite == False) & (knn_indices[i, j] == i):\n",
    "                val = 0.0\n",
    "            elif knn_dists[i, j] - rhos[i] <= 0.0 or sigmas[i] == 0.0:\n",
    "                val = 1.0\n",
    "            else:\n",
    "                val = np.exp(-((knn_dists[i, j] - rhos[i]) / (sigmas[i])))\n",
    "            rows[i * n_neighbors + j] = i\n",
    "            cols[i * n_neighbors + j] = knn_indices[i, j]\n",
    "            vals[i * n_neighbors + j] = val\n",
    "            dists[i * n_neighbors + j] = knn_dists[i, j]\n",
    "\n",
    "    return rows, cols, vals, dists\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "def fuzzy_simplicial_set(X, knn_indices, knn_dists, n_neighbors=15):\n",
    "    \"\"\"Given a set of data X, a neighborhood size, and a measure of distance\n",
    "    compute the fuzzy simplicial set (here represented as a fuzzy graph in\n",
    "    the form of a sparse matrix) associated to the data. This is done by\n",
    "    locally approximating geodesic distance at each point, creating a fuzzy\n",
    "    simplicial set for each such point, and then combining all the local\n",
    "    fuzzy simplicial sets into a global one via a fuzzy union.\n",
    "    \"\"\"\n",
    "    \n",
    "    knn_dists = knn_dists.astype(np.float32)\n",
    "    sigmas, rhos = smooth_knn_dist(knn_dists)\n",
    "    \n",
    "    \n",
    "    rows, cols, vals, dists = compute_membership_strengths(knn_indices, knn_dists, sigmas, rhos)\n",
    "\n",
    "    #Calculate the fuzzy_simplicial_set\n",
    "    Pij = scipy.sparse.coo_matrix((vals, (rows, cols)), shape=(X.shape[0], X.shape[0]))\n",
    "    Pij.eliminate_zeros()\n",
    "    \n",
    "    Pji = Pij.transpose()\n",
    "    prod_matrix = Pij.multiply(Pji) \n",
    "    P = (Pij + Pji - prod_matrix)\n",
    "    P.eliminate_zeros()\n",
    "    \n",
    "    #Calculate the fuzzy simplex distance matrix\n",
    "    dmat = scipy.sparse.coo_matrix((dists, (rows, cols)), shape=(X.shape[0], X.shape[0]))\n",
    "    dists = dmat.maximum(dmat.transpose()).todok()\n",
    "\n",
    "    return P, sigmas, rhos, dists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P, sigmas, rhos, dists = fuzzy_simplicial_set(X_train, knn_indices, knn_dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_layout(data, graph, dim):\n",
    "    \"\"\"Given a graph compute the spectral embedding of the graph. This is\n",
    "    simply the eigenvectors of the laplacian of the graph. Here we use the\n",
    "    normalized laplacian.\n",
    "    \"\"\"\n",
    "    diag_data = np.asarray(graph.sum(axis=0))\n",
    "    # Normalized Laplacian\n",
    "    I = scipy.sparse.identity(graph.shape[0], dtype=np.float64)\n",
    "    D = scipy.sparse.spdiags(1.0 / np.sqrt(diag_data), 0, graph.shape[0], graph.shape[0])\n",
    "    L = I - D * graph * D\n",
    "    k = dim + 1\n",
    "    eigenvalues, eigenvectors = scipy.sparse.linalg.eigsh(L, k, which=\"SM\")\n",
    "    order = np.argsort(eigenvalues)[1:k]\n",
    "    return eigenvectors[:, order]\n",
    "  \n",
    "def random_layout(data, graph, dim):\n",
    "    random_state = np.random.RandomState(1234)\n",
    "    return random_state.uniform(low=-10.0, high=10.0, size=(graph.shape[0], dim))\n",
    "  \n",
    "  \n",
    "def init_embeddings(X, P, n_components):\n",
    "  \n",
    "   #Spectral init\n",
    "  initialisation = spectral_layout(X, P, n_components)\n",
    "  \n",
    "  #Add noise\n",
    "  expansion = 10.0 / np.abs(initialisation).max()\n",
    "  random_state = check_random_state(0)\n",
    "  embedding = (initialisation * expansion).astype(np.float32) + random_state.normal(scale=0.0001, size=[P.shape[0], n_components]).astype(np.float32)\n",
    "  \n",
    "  embedding = (10.0 * (embedding - np.min(embedding, 0)) / (np.max(embedding, 0) - np.min(embedding, 0))).astype(np.float32, order=\"C\")\n",
    "  return embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adalmia1/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354260,)\n",
      "\tcompleted  0  /  800 epochs\n",
      "\tcompleted  80  /  800 epochs\n",
      "\tcompleted  160  /  800 epochs\n",
      "\tcompleted  240  /  800 epochs\n",
      "\tcompleted  320  /  800 epochs\n",
      "\tcompleted  400  /  800 epochs\n",
      "\tcompleted  480  /  800 epochs\n",
      "\tcompleted  560  /  800 epochs\n",
      "\tcompleted  640  /  800 epochs\n",
      "\tcompleted  720  /  800 epochs\n"
     ]
    }
   ],
   "source": [
    "INT32_MIN = np.iinfo(np.int32).min + 1\n",
    "INT32_MAX = np.iinfo(np.int32).max - 1\n",
    "\n",
    "\n",
    "def make_epochs_per_sample(weights, n_epochs):\n",
    "    \"\"\"Given a set of weights and number of epochs generate the number of\n",
    "    epochs per sample for each weight.\n",
    "    Returns\n",
    "    -------\n",
    "    An array of number of epochs per sample, one for each 1-simplex.\n",
    "    \"\"\"\n",
    "    result = -1.0 * np.ones(weights.shape[0], dtype=np.float64)\n",
    "    n_samples = n_epochs * (weights / weights.max())\n",
    "    \n",
    "    result[n_samples > 0] = float(n_epochs) / n_samples[n_samples > 0]\n",
    "    print(result.shape)\n",
    "    return result\n",
    "\n",
    "  \n",
    "@numba.njit()\n",
    "def clip(val):\n",
    "    \"\"\"Standard clamping of a value into a fixed range (in this case -4.0 to\n",
    "    4.0)\n",
    "    \"\"\"\n",
    "    if val > 4.0:\n",
    "        return 4.0\n",
    "    elif val < -4.0:\n",
    "        return -4.0\n",
    "    else:\n",
    "        return val  \n",
    "  \n",
    "\n",
    "  \n",
    "@numba.njit(\n",
    "    \"f4(f4[::1],f4[::1])\",\n",
    "    fastmath=True,\n",
    "    cache=True,\n",
    "    locals={\n",
    "        \"result\": numba.types.float32,\n",
    "        \"diff\": numba.types.float32,\n",
    "        \"dim\": numba.types.int32,\n",
    "    },\n",
    ")\n",
    "def rdist(x, y):\n",
    "    result = 0.0\n",
    "    dim = x.shape[0]\n",
    "    for i in range(dim):\n",
    "        diff = x[i] - y[i]\n",
    "        result += diff * diff\n",
    "\n",
    "    return result\n",
    "  \n",
    "  \n",
    "  \n",
    "@numba.njit(\"i4(i8[:])\")\n",
    "def tau_rand_int(state):\n",
    "    \"\"\"A fast (pseudo)-random number generator.\n",
    "    A (pseudo)-random int32 value\n",
    "    \"\"\"\n",
    "    state[0] = (((state[0] & 4294967294) << 12) & 0xFFFFFFFF) ^ (\n",
    "        (((state[0] << 13) & 0xFFFFFFFF) ^ state[0]) >> 19\n",
    "    )\n",
    "    state[1] = (((state[1] & 4294967288) << 4) & 0xFFFFFFFF) ^ (\n",
    "        (((state[1] << 2) & 0xFFFFFFFF) ^ state[1]) >> 25\n",
    "    )\n",
    "    state[2] = (((state[2] & 4294967280) << 17) & 0xFFFFFFFF) ^ (\n",
    "        (((state[2] << 3) & 0xFFFFFFFF) ^ state[2]) >> 11\n",
    "    )\n",
    "\n",
    "    return state[0] ^ state[1] ^ state[2]\n",
    "\n",
    "  \n",
    "def optimize_layout_euclidean_single_epoch(head_embedding, tail_embedding, head,tail,n_vertices,epochs_per_sample,a,b,rng_state,dim,move_other,alpha,epochs_per_negative_sample,epoch_of_next_negative_sample,epoch_of_next_sample,n):\n",
    "    for i in numba.prange(epochs_per_sample.shape[0]):\n",
    "        if epoch_of_next_sample[i] <= n:\n",
    "            j = head[i]\n",
    "            k = tail[i]\n",
    "\n",
    "            current = head_embedding[j]\n",
    "            other = tail_embedding[k]\n",
    "\n",
    "            \n",
    "            dist_squared = rdist(current, other)\n",
    "            \n",
    "            grad_coeff = -2.0 * a * b * pow(dist_squared, b - 1.0) \n",
    "            grad_coeff /= (a*pow(dist_squared, b) + 1.0)\n",
    "\n",
    "            for d in range(dim): \n",
    "                grad_d = clip(grad_coeff * (current[d] - other[d]))\n",
    "                current[d] += grad_d * alpha\n",
    "                if move_other:\n",
    "                    other[d] += -grad_d * alpha\n",
    "\n",
    "            epoch_of_next_sample[i] += epochs_per_sample[i]\n",
    "\n",
    "            n_neg_samples = int((n - epoch_of_next_negative_sample[i]) / epochs_per_negative_sample[i])\n",
    "\n",
    "            for p in range(n_neg_samples):\n",
    "              \n",
    "                k = tau_rand_int(rng_state) % n_vertices\n",
    "                other = tail_embedding[k]\n",
    "                dist_squared = rdist(current, other)\n",
    "\n",
    "                if dist_squared > 0.0:\n",
    "                    grad_coeff = 2.0  * b \n",
    "                    grad_coeff /= (0.001 + dist_squared) * (a * pow(dist_squared, b) + 1)\n",
    "                elif j == k:\n",
    "                    continue\n",
    "                else:\n",
    "                    grad_coeff = 0.0\n",
    "\n",
    "                for d in range(dim):\n",
    "                    if grad_coeff > 0.0:\n",
    "                        grad_d = clip(grad_coeff * (current[d] - other[d]))\n",
    "                    else:\n",
    "                        grad_d = 4.0\n",
    "                    current[d] += grad_d * alpha\n",
    "\n",
    "            epoch_of_next_negative_sample[i] += (n_neg_samples * epochs_per_negative_sample[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "def simplicial_set_embedding(X, P, n_components, initial_alpha, a, b, gamma, negative_sample_rate):\n",
    "    \"\"\"Perform a fuzzy simplicial set embedding, using a specified\n",
    "    initialisation method and then minimizing the fuzzy set cross entropy\n",
    "    between the 1-skeletons of the high and low dimensional fuzzy simplicial\n",
    "    sets.\n",
    "    \"\"\"\n",
    "    graph = P.tocoo()\n",
    "    graph.sum_duplicates()\n",
    "    n_vertices = graph.shape[1]\n",
    "\n",
    "    n_epochs = 800\n",
    "    \n",
    "    embedding = init_embeddings(X_train, P, n_components)\n",
    "    \n",
    "    #Make epochs\n",
    "    epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "   \n",
    "    \n",
    "    head = graph.row\n",
    "    tail = graph.col\n",
    "    weight = graph.data\n",
    "    \n",
    "    random_state = check_random_state(0)\n",
    "    rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)\n",
    "\n",
    "    \n",
    "    #Optimize Step\n",
    "    head_embedding = embedding\n",
    "    tail_embedding = embedding\n",
    "    \n",
    "    \n",
    "    move_other = head_embedding.shape[0] == tail_embedding.shape[0]\n",
    "    \n",
    "    \n",
    "    epochs_per_negative_sample = epochs_per_sample / negative_sample_rate\n",
    "    epoch_of_next_negative_sample = epochs_per_negative_sample.copy()\n",
    "    \n",
    "    #print(epoch_of_next_negative_sample)\n",
    "    \n",
    "    epoch_of_next_sample = epochs_per_sample.copy()\n",
    "    #print(epoch_of_next_sample)\n",
    "\n",
    "    \n",
    "    optimize_fn = numba.njit(optimize_layout_euclidean_single_epoch, fastmath=True, parallel=False)\n",
    "    \n",
    "    alpha = initial_alpha\n",
    "    \n",
    "\n",
    "    for n in range(n_epochs):\n",
    "\n",
    "\n",
    "        optimize_fn(\n",
    "            head_embedding,\n",
    "            tail_embedding,\n",
    "            head,\n",
    "            tail,\n",
    "            n_vertices,\n",
    "            epochs_per_sample,\n",
    "            a,\n",
    "            b,\n",
    "            rng_state,\n",
    "            n_components,\n",
    "            move_other,\n",
    "            alpha,\n",
    "            epochs_per_negative_sample,\n",
    "            epoch_of_next_negative_sample,\n",
    "            epoch_of_next_sample,\n",
    "            n\n",
    "        )\n",
    "\n",
    "        alpha = initial_alpha * (1.0 - (float(n) / float(n_epochs)))\n",
    "\n",
    "        if n % int(n_epochs / 10) == 0:\n",
    "            print(\"\\tcompleted \", n, \" / \", n_epochs, \"epochs\")\n",
    "\n",
    "    return head_embedding\n",
    "\n",
    "\n",
    "  \n",
    "embeds = simplicial_set_embedding(X_train, P, 400, 1, a, b, 1, 5)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = []\n",
    "colors += cm.get_cmap(\"Set3\").colors\n",
    "colors += cm.get_cmap(\"Set2\").colors\n",
    "my_cmap = ListedColormap(colors)\n",
    "\n",
    "plt.scatter(embeds[:, 0], embeds[:, 1], c = y_train.astype(int), cmap = my_cmap, s = 50)\n",
    "plt.title('UMAP', fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=20, random_state=2)\n",
    "y_pred_train = kmeans.fit_predict(embeds)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurary Score: 0.626\n",
      "Adjusted Mutual Information Score: 0.545\n",
      "Adjusted Rand Index Score: 0.438\n",
      "Normalized Mutual Information Score: 0.547\n",
      "Homogeneity: 0.531\n",
      "Completeness: 0.563\n",
      "V-measure: 0.547\n"
     ]
    }
   ],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) \n",
    "def evaluate(y, y_pred): \n",
    "  print(\"Accurary Score: %0.3f\" % purity_score(y, y_pred))\n",
    "  print(\"Adjusted Mutual Information Score: %0.3f\" % metrics.adjusted_mutual_info_score(y, y_pred))\n",
    "  print(\"Adjusted Rand Index Score: %0.3f\" % metrics.adjusted_rand_score(y, y_pred))\n",
    "  print(\"Normalized Mutual Information Score: %0.3f\" % metrics.normalized_mutual_info_score(y, y_pred))\n",
    "\n",
    "  print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(y, y_pred))\n",
    "  print(\"Completeness: %0.3f\" % metrics.completeness_score(y, y_pred))\n",
    "  print(\"V-measure: %0.3f\" % metrics.v_measure_score(y, y_pred))\n",
    "\n",
    "\n",
    "evaluate(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from umap import UMAP\n",
    "\n",
    "\n",
    "\n",
    "#model = UMAP(n_neighbors = 15, min_dist = 0.1, n_components = 2, verbose = True)\n",
    "#umap = model.fit_transform(X_train)\n",
    "#plt.scatter(umap[:, 0], umap[:, 1], c = y_train.astype(int), cmap = my_cmap, s = 50)\n",
    "#plt.title('UMAP', fontsize = 20)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
